= Compiling & Running 

== Start the Cluster

. To start the cluster execute the following command from within the folder `sample-app`:
+
[source,subs="verbatim,quotes"]
--
$ *docker-compose up -d*
--

. To exec into the `kafka` container execute:
+
[source,subs="verbatim,quotes"]
--
$ *docker-compose exec kafka /bin/bash*
root@kafka:/#
--

. To create the topic `cc-authorization-topic` execute:
+
[source,subs="verbatim,quotes"]
--
root@kafka:/# *kafka-topics --create \
    --zookeeper zookeeper:2181 \
    --partitions 3 \
    --replication-factor 1 \
    --topic cc-authorization-topic*
Created topic "cc-authorization-topic".
--

== Producer

. To build the consumer run the following command from the producer's folder:
+
[source,subs="verbatim,quotes"]
--
$ *docker container run --rm \
    -v "$PWD":/home/gradle/project \
    -v "$HOME"/.gradle-docker:/root/.gradle \
    -w /home/gradle/project \
    frekele/gradle:latest gradle build*
--

. To run the producer:
+
[source,subs="verbatim,quotes"]
--
$ *docker container run --rm \
    --net sample-app_app-net \
    -v "$PWD"/build/libs/app.jar:/app/app.jar \
    -v "$PWD"/producer.properties:/app/producer.properties \
    openjdk:11-jre-slim java -jar /app/app.jar*
--
+
You should see something like this:
+
[source]
--
*** Starting CC Authorization Producer ***
............e.e......................e............e............................e.......e...e...............................e....e..e........e......ee..........e..e........e..e.........................e...e.......................e....e..e.........e.................e..ee..............FRAUD................e....e.e................................e....e.........................e.e.......
--
+
Note the *FRAUD* on the third line. This indicates that a pattern for a potential fraud has been generated by the producer.

. To read the data produced by the producer use this command from within the `kafka` container:
+
[source,subs="verbatim,quotes"]
--
$ docker-compose exec schema-registry /bin/bash*
root@schema-registry:/# *kafka-avro-console-consumer \
    --bootstrap-server kafka:9092 \
    --from-beginning \
    --property schema.registry.url=http://schema-registry:8081 \
    --topic cc-authorization-topic*
...
{"credit_card_nbr":"cc-a0000004185","auth_time":1543328421314,"status":"OK"}
{"credit_card_nbr":"cc-a0000009249","auth_time":1543328421537,"status":"FAILED"}
{"credit_card_nbr":"cc-a0000002113","auth_time":1543328422094,"status":"OK"}
{"credit_card_nbr":"cc-a0000001879","auth_time":1543328422206,"status":"OK"}
...
--

== Converter

. Exec into the KSQL CLI and connect to KSQL Server with this command:
+
[source,subs="verbatim,quotes"]
--
$ *docker-compose exec ksql-cli ksql http://ksql-server:8088*
--

. Show all topics visible to you:
+
[source,subs="verbatim,quotes"]
--
ksql> *SHOW topics;*
--

. Create a stream `cc_authorizations` with this command:
+
[source,subs="verbatim,quotes"]
--
ksql> *CREATE STREAM cc_authorizations( \
    credit_card_nbr STRING, \
    auth_time BIGINT, \
    status STRING) \
    WITH(KAFKA_TOPIC='cc-authorization-topic', VALUE_FORMAT='AVRO', KEY='credit_card_nbr');*
--

. Make sure the queries return all data from the very beginning of the underlying topics:
+
[source,subs="verbatim,quotes"]
--
ksql> *set 'auto.offset.reset'='earliest';*
--

. Define this quert to discover any frauds:
+
[source,subs="verbatim,quotes"]
--
ksql> *CREATE TABLE CC_POTENTIAL_FRAUD AS \
    SELECT credit_card_nbr, COUNT( * ) attempts \
    FROM cc_authorizations \
    WINDOW TUMBLING (SIZE 3 SECONDS) \
    WHERE status='FAILED' \
    GROUP BY credit_card_nbr \
    HAVING COUNT( * )>=3;*
--

. And to query the table use:
+
[source,subs="verbatim,quotes"]
--
ksql> *SELECT credit_card_nbr, attempts \
    from CC_POTENTIAL_FRAUD;*
--

. Create an intermediate stream:
+
[source,subs="verbatim,quotes"]
--
ksql> *CREATE STREAM CC_POTENTIAL_FRAUD_STREAM (\
    credit_card_nbr string, attempts bigint) \
    WITH (kafka_topic='CC_POTENTIAL_FRAUD', value_format='AVRO');*
--

. And then filter this stream:
+
[source,subs="verbatim,quotes"]
--
ksql> *CREATE STREAM CC_POTENTIAL_FRAUD_COUNTS \
    WITH (PARTITIONS=6,REPLICAS=1) AS \
    SELECT * FROM CC_POTENTIAL_FRAUD_STREAM WHERE ROWTIME IS NOT NULL;*
--

. Analyze the content of the underlying topic `CC_POTENTIAL_FRAUD_COUNTS`:
+
[source,subs="verbatim,quotes"]
--
$ *docker-compose exec schema-registry /bin/bash*
root@schema-registry:/# *kafka-avro-console-consumer \
    --bootstrap-server kafka:9092 \
    --from-beginning \
    --property schema.registry.url=http://schema-registry:8081 \
    --topic CC_POTENTIAL_FRAUD_COUNTS*
{"CREDIT_CARD_NBR":{"string":"cc-a0000000345"},"ATTEMPTS":{"long":3}}
{"CREDIT_CARD_NBR":{"string":"cc-a0000001168"},"ATTEMPTS":{"long":3}}
{"CREDIT_CARD_NBR":{"string":"cc-a0000002694"},"ATTEMPTS":{"long":3}}
{"CREDIT_CARD_NBR":{"string":"cc-a0000005202"},"ATTEMPTS":{"long":3}}
...
--

== Consumer


. Run the Microsoft .NET Core (with SDK) container interactively:
+
[source,subs="verbatim,quotes"]
--
$ *cd ~/confluent-dev/labs/kafka-avro/dotnet/converter*
$ *docker container run --rm -it \
    --hostname dotnet \
    --net sample-app_app-net \
    -v ${HOME}/.nuget-docker:/root/.nuget \
    -v ${HOME}/.dotnet-docker/tools:/root/.dotnet/tools \
    -v $(pwd):/app \
    -w /app \
    -p 5000:5000 \
    microsoft/dotnet:2.1-sdk /bin/bash*
--

. Install the `AvroGen` tool that will generate the C# classes from Avro schemas:
+
[source,subs="verbatim,quotes"]
--
root@dotnet:/app# *dotnet tool install -g Confluent.Apache.Avro.AvroGen*
--

. Add the folder `/root/.dotnet/tools` to the `PATH` variable:
+
[source,subs="verbatim,quotes"]
--
root@dotnet:/app# *export PATH="$PATH:/root/.dotnet/tools"*
--

. Create the two C# classes from the Avro schemas as follows:
+
[source,subs="verbatim,quotes"]
--
root@dotnet:/app# *avrogen -s FraudValue.cs.asvc .*
--
